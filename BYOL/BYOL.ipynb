{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18a2adcfc50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define models and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EMA(new, alpha=0.99, old=None):\n",
    "    if old is None:\n",
    "        return new\n",
    "    else:\n",
    "        return old * alpha + (1 - alpha) * new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x, y):\n",
    "   # L2 normalization\n",
    "   x = F.normalize(x, dim=-1, p=2)\n",
    "   y = F.normalize(y, dim=-1, p=2)\n",
    "   return 2 - 2 * (x * y).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=2048) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 256)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(TargetModel, self).__init__()\n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.encoder.maxpool = torch.nn.Identity()\n",
    "\n",
    "        self.represent = MLP()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.represent(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(OnlineModel, self).__init__()\n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        self.encoder.maxpool = torch.nn.Identity()\n",
    "\n",
    "        self.represent = MLP()\n",
    "\n",
    "        # self.predictor = MLP(input_dim=256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.represent(x)\n",
    "        # x = self.predictor(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYOL(nn.Module):\n",
    "    def __init__(self,\n",
    "                 moving_average_decay=0.99) -> None:\n",
    "        super(BYOL, self).__init__()\n",
    "\n",
    "        self.student_model = OnlineModel()\n",
    "        self.teacher_model = TargetModel()\n",
    "        self.moving_average_decay = moving_average_decay\n",
    "        self.student_predictor = MLP(input_dim=256)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_moving_average(self):\n",
    "       assert self.teacher_model is not None, 'target encoder has not been created yet'\n",
    "       for student_params, teacher_params in zip(self.student_model.parameters(), self.teacher_model.parameters()):\n",
    "         old_weight, up_weight = teacher_params.data, student_params.data\n",
    "         teacher_params.data = EMA(old=old_weight, new=up_weight, alpha=self.moving_average_decay)\n",
    "\n",
    "    def forward(self,\n",
    "                image1,\n",
    "                image2):\n",
    "       # student projections: backbone + MLP projection\n",
    "       student_proj_one = self.student_model(image1)\n",
    "       student_proj_two = self.student_model(image2)\n",
    "\n",
    "       # additional student's MLP head called predictor\n",
    "       student_pred_one = self.student_predictor(student_proj_one)\n",
    "       student_pred_two = self.student_predictor(student_proj_two)\n",
    "\n",
    "       with torch.no_grad():\n",
    "           # teacher processes the images and makes projections: backbone + MLP\n",
    "           teacher_proj_one = self.teacher_model(image1).detach_()\n",
    "           teacher_proj_two = self.teacher_model(image2).detach_()\n",
    "\n",
    "\n",
    "       loss_one = loss_fn(student_pred_one, teacher_proj_one)\n",
    "       loss_two = loss_fn(student_pred_two, teacher_proj_two)       \n",
    "\n",
    "       return (loss_one + loss_two).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data + augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.3, 0.1)], p=0.5),\n",
    "                                transforms.RandomGrayscale(p=0.5),\n",
    "                                transforms.RandomApply([transforms.GaussianBlur(3)], p=0.5), \n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_train = datasets.CIFAR10(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset_train, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "byol = BYOL()\n",
    "opt = torch.optim.Adam(byol.parameters(), lr=0.003)\n",
    "epochs = 20\n",
    "byol.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in target_model.parameters():\n",
    "#     p.require_grads = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/391 [00:00<?, ?it/s]e:\\2023_processing_and_generating_images_course_homeworks\\venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████| 391/391 [01:36<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.03737498081677482\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:36<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0019414913358257325\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:35<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.000578703972852796\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:33<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0001380612016255048\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:34<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 3.2501262820684865e-05\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:34<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 9.236966386134506e-06\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:35<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.8549330362798627e-06\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:35<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 8.169756940113358e-07\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:36<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 5.285505710355461e-07\n",
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:36<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 4.894898065825557e-07\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: %d' % (epoch + 1))\n",
    "    count = 0\n",
    "    train_loss = 0\n",
    "    for x, y in tqdm.tqdm(train_loader):\n",
    "        count += 1\n",
    "        opt.zero_grad()\n",
    "\n",
    "        x1 = transform(x).to(device).float()\n",
    "        x2 = transform(x).to(device).float()\n",
    "\n",
    "        loss = byol(x1, x2)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        byol.update_moving_average()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    losses.append(train_loss / count)\n",
    "    print(f\"train_loss: {train_loss / count}\")\n",
    "    torch.save(byol.student_model.encoder.state_dict(), f\"./pretrained_feature_extractors/feature_extractor_{epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses, label='train_loss')\n",
    "plt.legend()\n",
    "plt.savefig(f\"./pretrained_feature_extractors/train_loss\", bbox_inches='tight')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
